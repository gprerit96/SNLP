{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tempfile\n",
    "import pickle,os\n",
    "\n",
    "from os import remove\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "try:\n",
    "    from numpy import array\n",
    "    from scipy import sparse\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn import linear_model\n",
    "    from sklearn import svm\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from nltk.parse import ParserI, DependencyGraph, DependencyEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Stack : ' + \\\n",
    "            str(self.stack) + '  Buffer : ' + str(self.buffer) + '   Arcs : ' + str(self.arcs)\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transition(object):\n",
    "    \"\"\"\n",
    "    This class defines a set of transition which is applied to a configuration to get another configuration\n",
    "    Note that for different parsing algorithm, the transition is different.\n",
    "    \"\"\"\n",
    "    # Define set of transitions\n",
    "    LEFT_ARC = 'LEFTARC'\n",
    "    RIGHT_ARC = 'RIGHTARC'\n",
    "    SHIFT = 'SHIFT'\n",
    "    REDUCE = 'REDUCE'\n",
    "\n",
    "    def __init__(self, alg_option):\n",
    "        \"\"\"\n",
    "        :param alg_option: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type alg_option: str\n",
    "        \"\"\"\n",
    "        self._algo = alg_option\n",
    "        if alg_option not in [\n",
    "                TransitionParser.ARC_STANDARD,\n",
    "                TransitionParser.ARC_EAGER]:\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER))\n",
    "\n",
    "\n",
    "    def left_arc(self, conf, relation):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "            return -1\n",
    "        if conf.buffer[0] == 0:\n",
    "            # here is the Root element\n",
    "            return -1\n",
    "\n",
    "        idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "\n",
    "        flag = True\n",
    "        if self._algo == TransitionParser.ARC_EAGER:\n",
    "            for (idx_parent, r, idx_child) in conf.arcs:\n",
    "                if idx_child == idx_wi:\n",
    "                    flag = False\n",
    "\n",
    "        if flag:\n",
    "            conf.stack.pop()\n",
    "            idx_wj = conf.buffer[0]\n",
    "            conf.arcs.append((idx_wj, relation, idx_wi))\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def right_arc(self, conf, relation):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for right-arc is DIFFERENT for arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "            return -1\n",
    "        if self._algo == TransitionParser.ARC_STANDARD:\n",
    "            idx_wi = conf.stack.pop()\n",
    "            idx_wj = conf.buffer[0]\n",
    "            conf.buffer[0] = idx_wi\n",
    "            conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "        else:  # arc-eager\n",
    "            idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "            idx_wj = conf.buffer.pop(0)\n",
    "            conf.stack.append(idx_wj)\n",
    "            conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "\n",
    "\n",
    "    def reduce(self, conf):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for reduce is only available for arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "\n",
    "        if self._algo != TransitionParser.ARC_EAGER:\n",
    "            return -1\n",
    "        if len(conf.stack) <= 0:\n",
    "            return -1\n",
    "\n",
    "        idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "        flag = False\n",
    "        for (idx_parent, r, idx_child) in conf.arcs:\n",
    "            if idx_child == idx_wi:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            conf.stack.pop()  # reduce it\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def shift(self, conf):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for shift is the SAME for arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if len(conf.buffer) <= 0:\n",
    "            return -1\n",
    "        idx_wi = conf.buffer.pop(0)\n",
    "        conf.stack.append(idx_wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not(algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\" Currently we only support %s and %s \" %\n",
    "                                        (self.ARC_STANDARD, self.ARC_EAGER))\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(str(featureID) + ':1.0' for featureID in sorted(unsorted_result))\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(\n",
    "                                key,\n",
    "                                binary_features,\n",
    "                                input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, modelname, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        :param modelname : 'SVM','logistic','MLP'\n",
    "        :type modelname : str \n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train',\n",
    "                dir=tempfile.gettempdir(),\n",
    "                delete=False)\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            \n",
    "            if(modelname=='SVM'):\n",
    "        \n",
    "                # The parameter is set according to the paper:\n",
    "                # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "                # Todo : because of probability = True => very slow due to\n",
    "                # cross-validation. Need to improve the speed here\n",
    "                model = svm.SVC(\n",
    "                    kernel='poly',\n",
    "                    degree=2,\n",
    "                    coef0=0,\n",
    "                    gamma=0.2,\n",
    "                    C=0.5,\n",
    "                    verbose=verbose,\n",
    "                    probability=True)\n",
    "                \n",
    "            if(modelname=='logistic'):\n",
    "                \n",
    "                model = linear_model.LogisticRegression(\n",
    "                    C = 0.5,\n",
    "                    solver = 'lbfgs',\n",
    "                    verbose = verbose)\n",
    "                \n",
    "            if(modelname=='MLP'):\n",
    "                \n",
    "                model = MLPClassifier(\n",
    "                     hidden_layer_sizes=(100,50,),\n",
    "                    learning_rate = 'adaptive',\n",
    "                    max_iter=1000)\n",
    "        finally:\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "            remove(input_file.name)\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix((np_data, (np_row, np_col)), shape=(1, len(self._dictionary)))\n",
    "\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(\n",
    "                    prob_dict.items(),\n",
    "                    key=itemgetter(1),\n",
    "                    reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    #y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if operation.left_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if operation.right_arc(conf, strTransition.split(\":\")[1]) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\"The predicted transition is not recognized, expected errors\")\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating files inclusive and exclusive of Morphological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train Data\n",
    "f1 = open('train_sent_m.conllu',\"w+\")\n",
    "f2 = open('train_sent_nm.conllu',\"w+\")\n",
    "with open(\"UD_Hindi/hi-ud-train.conllu\",\"r+\",encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if(line == '\\n'):\n",
    "            f1.write(line)\n",
    "            f2.write(line)\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            line1 = list(line)\n",
    "            line1[5] = line1[5]+\"|\"+line1[9][:-1]\n",
    "            line[5] = \"_\"\n",
    "            line = '\\t'.join(line)\n",
    "            line1 = '\\t'.join(line1)\n",
    "            f1.write(line1)\n",
    "            f2.write(line)\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "#Test Data\n",
    "f1 = open('test_sent_m.conllu',\"w+\")\n",
    "f2 = open('test_sent_nm.conllu',\"w+\")\n",
    "with open(\"UD_Hindi/hi-ud-test.conllu\",\"r+\",encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if(line == '\\n'):\n",
    "            f1.write(line)\n",
    "            f2.write(line)\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            line1 = list(line)\n",
    "            line1[5] = line1[5]+\"|\"+line1[9][:-1]\n",
    "            line[5] = \"_\"\n",
    "            line = '\\t'.join(line)\n",
    "            line1 = '\\t'.join(line1)\n",
    "            f1.write(line1)\n",
    "            f2.write(line)\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing models with Morphological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prerit/anaconda/envs/py36/lib/python3.6/site-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "train_sent_m = DependencyGraph.load(\"train_sent_m.conllu\")\n",
    "test_sent_m = DependencyGraph.load(\"test_sent_m.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Arc-Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-standard')\n",
    "parser_std.train(train_sent_m,'temp.arcstd_svm_m.model','SVM', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.486000 %\n",
      "UAS : 93.322000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_m, 'temp.arcstd_svm_m.model')\n",
    "de = DependencyEvaluator(result, train_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 91.308000 %\n",
      "UAS : 83.296000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_m, 'temp.arcstd_svm_m.model')\n",
    "de = DependencyEvaluator(result, test_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-standard')\n",
    "parser_std.train(train_sent_m,'temp.arcstd_log_m.model','logistic', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 95.529000 %\n",
      "UAS : 89.052000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_m, 'temp.arcstd_log_m.model')\n",
    "de = DependencyEvaluator(result, train_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 86.697000 %\n",
      "UAS : 76.720000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_m, 'temp.arcstd_log_m.model')\n",
    "de = DependencyEvaluator(result, test_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-standard')\n",
    "parser_std.train(train_sent_m,'temp.arcstd_mlp_m.model','MLP', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.586000 %\n",
      "UAS : 93.538000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_m, 'temp.arcstd_mlp_m.model')\n",
    "de = DependencyEvaluator(result, train_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 86.621000 %\n",
      "UAS : 77.022000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_m, 'temp.arcstd_mlp_m.model')\n",
    "de = DependencyEvaluator(result, test_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### By Arc-Eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-eager')\n",
    "parser_std.train(train_sent_m,'temp.arceag_svm_m.model','SVM', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.558000 %\n",
      "UAS : 93.480000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_m, 'temp.arceag_svm_m.model')\n",
    "de = DependencyEvaluator(result, train_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 90.854000 %\n",
      "UAS : 82.540000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_m, 'temp.arceag_svm_m.model')\n",
    "de = DependencyEvaluator(result, test_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-eager')\n",
    "parser_std.train(train_sent_m,'temp.arceag_log_m.model','logistic', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 96.106000 %\n",
      "UAS : 90.004000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_m, 'temp.arceag_log_m.model')\n",
    "de = DependencyEvaluator(result, train_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 90.249000 %\n",
      "UAS : 80.272000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_m, 'temp.arceag_log_m.model')\n",
    "de = DependencyEvaluator(result, test_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-eager')\n",
    "parser_std.train(train_sent_m,'temp.arceag_mlp_m.model','MLP', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.687000 %\n",
      "UAS : 93.581000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_m, 'temp.arceag_mlp_m.model')\n",
    "de = DependencyEvaluator(result, train_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 85.034000 %\n",
      "UAS : 75.888000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_m, 'temp.arceag_mlp_m.model')\n",
    "de = DependencyEvaluator(result, test_sent_m)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing models without Morphological Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prerit/anaconda/envs/py36/lib/python3.6/site-packages/nltk/parse/dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "train_sent_nm = DependencyGraph.load(\"train_sent_nm.conllu\")\n",
    "test_sent_nm = DependencyGraph.load(\"test_sent_nm.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Arc-Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-standard')\n",
    "parser_std.train(train_sent_nm,'temp.arcstd_svm_nm.model','SVM', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 97.937000 %\n",
      "UAS : 92.629000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_nm, 'temp.arcstd_svm_nm.model')\n",
    "de = DependencyEvaluator(result, train_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 84.732000 %\n",
      "UAS : 76.266000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_nm, 'temp.arcstd_svm_nm.model')\n",
    "de = DependencyEvaluator(result, test_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-standard')\n",
    "parser_std.train(train_sent_nm,'temp.arcstd_log_nm.model','logistic', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 91.086000 %\n",
      "UAS : 81.898000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_nm, 'temp.arcstd_log_nm.model')\n",
    "de = DependencyEvaluator(result, train_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 79.289000 %\n",
      "UAS : 68.178000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_nm, 'temp.arcstd_log_nm.model')\n",
    "de = DependencyEvaluator(result, test_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-standard')\n",
    "parser_std.train(train_sent_nm,'temp.arcstd_mlp_nm.model','MLP', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.183000 %\n",
      "UAS : 92.889000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_nm, 'temp.arcstd_mlp_nm.model')\n",
    "de = DependencyEvaluator(result, train_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 79.063000 %\n",
      "UAS : 68.707000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_nm, 'temp.arcstd_mlp_nm.model')\n",
    "de = DependencyEvaluator(result, test_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### By Arc-Eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-eager')\n",
    "parser_std.train(train_sent_nm,'temp.arceag_svm_nm.model','SVM', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.096000 %\n",
      "UAS : 92.788000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_nm, 'temp.arceag_svm_nm.model')\n",
    "de = DependencyEvaluator(result, train_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 87.075000 %\n",
      "UAS : 77.324000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_nm, 'temp.arceag_svm_nm.model')\n",
    "de = DependencyEvaluator(result, test_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-eager')\n",
    "parser_std.train(train_sent_nm,'temp.arceag_log_nm.model','logistic', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 92.471000 %\n",
      "UAS : 83.874000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_nm, 'temp.arceag_log_nm.model')\n",
    "de = DependencyEvaluator(result, train_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 84.354000 %\n",
      "UAS : 72.789000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_nm, 'temp.arceag_log_nm.model')\n",
    "de = DependencyEvaluator(result, test_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 501\n",
      " Number of valid (projective) examples : 477\n"
     ]
    }
   ],
   "source": [
    "####### Training ######\n",
    "input_file = tempfile.NamedTemporaryFile(prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False)\n",
    "parser_std = TransitionParser('arc-eager')\n",
    "parser_std.train(train_sent_nm,'temp.arceag_mlp_nm.model','MLP', verbose=False)\n",
    "remove(input_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 98.413000 %\n",
      "UAS : 93.091000 %\n"
     ]
    }
   ],
   "source": [
    "####### Training Accuracy ########\n",
    "result = parser_std.parse(train_sent_nm, 'temp.arceag_mlp_nm.model')\n",
    "de = DependencyEvaluator(result, train_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS : 83.296000 %\n",
      "UAS : 71.353000 %\n"
     ]
    }
   ],
   "source": [
    "####### Testing Accuracy ########\n",
    "result = parser_std.parse(test_sent_nm, 'temp.arceag_mlp_nm.model')\n",
    "de = DependencyEvaluator(result, test_sent_nm)\n",
    "las,uas = de.eval()\n",
    "print (\"LAS : %f\"%(round(las*100,3)),'%')\n",
    "print (\"UAS : %f\"%(round(uas*100,3)),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
