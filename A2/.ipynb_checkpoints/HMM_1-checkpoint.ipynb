{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the transition, start and emission matrix . The states stand for high and low.  The HMM model is given in the assignment itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P= np.array([[0.6, 0.4],[0.5,0.5]])\n",
    "\n",
    "S= np.array([0.5, 0.5])\n",
    "\n",
    "O= np.array([[0.3,0.2,0.2,0.3],[0.2,0.3,0.3,0.2]])\n",
    "\n",
    "state={}\n",
    "state[0]='L'\n",
    "state[1]='H'\n",
    "\n",
    "DNA={}\n",
    "DNA['A']=0\n",
    "DNA['C']=1\n",
    "DNA['G']=2\n",
    "DNA['T']=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_len= len(S)\n",
    "\n",
    "def viterbi_algo(sequence):\n",
    "    M = len(sequence)\n",
    "    \n",
    "    T = np.zeros((state_len,M))\n",
    "    for i in range(state_len):\n",
    "        T[i][0] = S[i]*O[i,DNA[sequence[0]]]\n",
    "    for j in range(1,M):\n",
    "        for i in range(state_len):\n",
    "            for k in range(state_len):\n",
    "                T[i][j] += T[k][j-1]*P[k,i]*O[i,DNA[sequence[j]]]\n",
    "    states = np.zeros(M)\n",
    "    states[-1] = np.argmax(T[:][-1])\n",
    "    print (states)\n",
    "    probs = np.zeros(state_len)\n",
    "    for j in reversed(range(M-1)):\n",
    "        for i in range(state_len):\n",
    "            probs[i] = T[i,j]*P[i,states[j+1]]*O[states[j+1],DNA[sequence[j+1]]]\n",
    "        states[j] = np.argmax(probs)\n",
    "\n",
    "    return (states,T[states[-1]][-1])            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8c893ee014c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mviterbi_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ba8238bbb680>\u001b[0m in \u001b[0;36mviterbi_algo\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDNA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "sequences=['GGC','GGCAAGATCAT','GAGAGGAGAGAGAGAGAGA']\n",
    "\n",
    "import time\n",
    "for sequence in sequences:\n",
    "    \n",
    "    t=time.time()\n",
    "    best=viterbi_algo(sequence)\n",
    "    t2=time.time()-t\n",
    "    \n",
    "    print('For the sequence '+ sequence+ ' of length '+ str(len(sequence))+' time taken was '+ str(round(t2,3))+'s' )\n",
    "    print('The sequence '+ ','.join([state[k] for k in best[0]])+ ' gave the best score of '+ str(best[1]))\n",
    "    #print (best)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A stupid attempt to show you why the exhaustive search is a bad, bad option for HMM modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import time \n",
    "def exhaustive_search(sequence):\n",
    "    \n",
    "    M= len(sequence)\n",
    "    state_len= len(S)\n",
    "    \n",
    "    # track the best sequence and its score\n",
    "    best=(None,float('-inf'))\n",
    "    \n",
    "    # basically loop will run for |states|^M \n",
    "    for ss in product(range(state_len),repeat=M):\n",
    "        \n",
    "        score= S[ss[0]]*O[ss[0],DNA[sequence[0]]]\n",
    "        \n",
    "        for i in range(1,M):\n",
    "            score*= P[ss[i-1],ss[i]]*O[ss[i],DNA[sequence[i]]]\n",
    "            \n",
    "        \n",
    "        #print(','.join([state[k] for k in ss]),score)\n",
    "    \n",
    "        if score > best[1]:\n",
    "            best= (ss,score)\n",
    "    \n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the sequence GGC of length 3 time taken was 0.0s\n",
      "The sequence H,H,H gave the best score of 0.003375\n",
      "\n",
      "\n",
      "For the sequence GGCAAGATCAT of length 11 time taken was 0.128s\n",
      "The sequence H,H,H,L,L,L,L,L,L,L,L gave the best score of 1.3774950719999997e-09\n",
      "\n",
      "\n",
      "For the sequence GAGAGGAGAGAGAGAGAGA of length 19 time taken was 32.839s\n",
      "The sequence H,L,L,L,H,H,L,L,L,L,L,L,L,L,L,L,L,L,L gave the best score of 1.3326697514029538e-16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequences=['GGC','GGCAAGATCAT','GAGAGGAGAGAGAGAGAGA']\n",
    "\n",
    "import time\n",
    "for sequence in sequences:\n",
    "    \n",
    "    t=time.time()\n",
    "    best=exhaustive_search(sequence)\n",
    "    t2=time.time()-t\n",
    "    \n",
    "    print('For the sequence '+ sequence+ ' of length '+ str(len(sequence))+' time taken was '+ str(round(t2,3))+'s' )\n",
    "    print('The sequence '+ ','.join([state[k] for k in best[0]])+ ' gave the best score of '+ str(best[1]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for this assignment: Brown corpus tagged with the Universal Tagset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will be your training set. The remaining 100 sentences will be used as your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55907\n",
      "12\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank,brown\n",
    "\n",
    "corpus = brown.tagged_sents(tagset='universal')[:-100] \n",
    "\n",
    "tag_dict={}\n",
    "word_dict={}\n",
    "\n",
    "for sent in corpus:\n",
    "    for elem in sent:\n",
    "        w = elem[0]\n",
    "        tag= elem[1]\n",
    "\n",
    "        if w not in word_dict:\n",
    "            word_dict[w]=0\n",
    "\n",
    "        if tag not in tag_dict:\n",
    "            tag_dict[tag]=0\n",
    "\n",
    "        word_dict[w]+=1\n",
    "        tag_dict[tag]+=1\n",
    "\n",
    "print(len(word_dict))\n",
    "print(len(tag_dict))\n",
    "        \n",
    "test_data= brown.tagged_sents(tagset='universal')[-10:]\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DET': 136724,\n",
       " 'NOUN': 275075,\n",
       " 'ADJ': 83581,\n",
       " 'VERB': 182380,\n",
       " 'ADP': 144483,\n",
       " '.': 147231,\n",
       " 'ADV': 56115,\n",
       " 'CONJ': 38067,\n",
       " 'PRT': 29759,\n",
       " 'PRON': 49174,\n",
       " 'NUM': 14853,\n",
       " 'X': 1369}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module to implement CRF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip3 install sklearn-crfsuite # install this please\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "train_sents= corpus\n",
    "\n",
    "def word2features(sent,i):\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features ={\n",
    "    'bias': 1.0,\n",
    "    }\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent,i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for i,label in sent]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=[sent2features(s) for s in train_sents]\n",
    "y_train=[sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test=[sent2features(s) for s in test_data]\n",
    "y_test=[sent2labels(s) for s in test_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12985271687027178"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "labels=list(crf.classes_)\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .      0.800     0.242     0.372        33\n",
      "          X      0.023     1.000     0.044         3\n",
      "        ADJ      0.000     0.000     0.000        18\n",
      "        ADP      0.179     0.185     0.182        27\n",
      "        ADV      0.000     0.000     0.000         9\n",
      "       VERB      0.000     0.000     0.000        35\n",
      "        DET      0.121     0.121     0.121        33\n",
      "       CONJ      0.000     0.000     0.000         7\n",
      "       NOUN      0.242     0.157     0.190        51\n",
      "       PRON      0.000     0.000     0.000        12\n",
      "        PRT      0.000     0.000     0.000        11\n",
      "        NUM      0.000     0.000     0.000         0\n",
      "\n",
      "avg / total      0.199     0.117     0.130       239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
