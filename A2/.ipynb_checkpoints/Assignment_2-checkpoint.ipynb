{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "### Speech & NLP\n",
    "\n",
    "### Name : Prerit Gupta, Roll No: 14EC35001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "#### Generative Models\n",
    "\n",
    "(a) Naive’s Bayes Classifier: The NB Classifier is based on the Baye’s theorum : p(y|x) = p(x|y)p(y)/p(x)  = p(x,y)/p(x), but it doesn’t model the conditional probability directly. It models the joint probability, and after that it calculates p(y|x).  This model can be used to generate data on the basis of p(x,y) modelled.\n",
    "\n",
    "(b) Gaussian Mixture Model: It represents the mixture distributions or joint distributions on gaussian distributed assumption of variables. A GMM can be used to generate data from the joint gaussian distribution of variable modelled.\n",
    "\n",
    "(c) GANs (Generative Adversarial Networks) : The generator network of the GAN models the underlying distribution of the supplied real data and becomes profidient in learning the true distribution of the real dataset by fooling the distinguishing power of the discriminator. Hence, GAN is a generative model used to generate distribution of a given dataset.\n",
    " \n",
    "(d) LDA(Latent Dirichlet Allocation): It is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.  \n",
    "\n",
    "#### Discriminative Models\n",
    "(a) Neural Networks: In neural networks, the likelihood and log-likelihood objective functions are both equivalent to the probability distribution p(y|x) as follows: L(θ) = L(θ;X,y) = L(y|X,θ) where the model θ is chosen to make the p(y|x) as high probability as possible. The task objective is classification of input data which is discrimination among possible class outcomes.\n",
    "\n",
    "(b) Logistic Regression: Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logisitic function (cumulative logical distribution). Logistic regression predicts the probability of particular outcomes rather than the outcomes themselves.\n",
    "\n",
    "(c) SVM (Support Vector Machine): Discriminative classifiers model the posterior (i.e. probability of class  given data, Pr(y|x)) directly from the data. SVM is discriminative because it  explicitly learns the class boundary between the two classes.\n",
    "\n",
    "(d) Decision Tree : A decision tree uses a tree-like graph to perform classification. It is discriminative as it distinguishes the class label of sampe conditioned on its underlying feature attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55907\n",
      "12\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank,brown\n",
    "\n",
    "corpus = brown.tagged_sents(tagset='universal')[:-100] \n",
    "\n",
    "tag_dict={}\n",
    "word_dict={}\n",
    "\n",
    "for sent in corpus:\n",
    "    for elem in sent:\n",
    "        w = elem[0]\n",
    "        tag= elem[1]\n",
    "\n",
    "        if w not in word_dict:\n",
    "            word_dict[w]=0\n",
    "\n",
    "        if tag not in tag_dict:\n",
    "            tag_dict[tag]=0\n",
    "\n",
    "        word_dict[w]+=1\n",
    "        tag_dict[tag]+=1\n",
    "\n",
    "print(len(word_dict))\n",
    "print(len(tag_dict))\n",
    "        \n",
    "test_data= brown.tagged_sents(tagset='universal')[-10:]\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12215.  8088.  1966.  2581.  7031.  5095.  5219.  2814.  2098.  9142.\n",
      "   961.    30.]\n"
     ]
    }
   ],
   "source": [
    "state_len = len(tag_dict)\n",
    "word_len = len(word_dict)\n",
    "state = {}\n",
    "i = 0\n",
    "for s in tag_dict.keys():\n",
    "    state[s] = i\n",
    "    i = i+1\n",
    "word = {}\n",
    "i = 0\n",
    "for w in word_dict.keys():\n",
    "    word[w] = i\n",
    "    i = i+1 \n",
    "P = np.zeros((state_len,state_len))\n",
    "S = np.zeros(state_len)\n",
    "O = np.zeros((state_len,word_len))\n",
    "for sent in corpus:\n",
    "    S[state[sent[0][1]]]+=1\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57240.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [ss/S.sum() for ss in S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
